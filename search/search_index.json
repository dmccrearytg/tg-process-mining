{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TigerGraph Process Mining","text":"<p>Welcome to the TigerGraph Process Mining website.</p> <p>Graph databases are an excellent fit for process mining applications because they are designed to handle highly interconnected data, making it easy to model and analyze complex business processes. Unlike traditional relational databases, which can struggle with the intricate relationships and dependencies inherent in process data, graph databases like TigerGraph excel at traversing relationships quickly and efficiently. This allows for real-time analysis of process flows, enabling organizations to identify bottlenecks, deviations, and optimization opportunities with greater precision.</p> <p>The ability to easily scale and handle large volumes of event data further enhances the suitability of graph databases for process mining, providing a powerful tool for uncovering actionable insights in even the most complex process landscapes.</p>"},{"location":"#predicting-workflow-times","title":"Predicting Workflow Times","text":"<p>Machine learning, particularly when combined with Graph Neural Networks (GNNs), can significantly enhance process mining by predicting the time required to complete a business process. GNNs are specifically designed to work with graph-structured data, making them ideal for capturing the dependencies and sequential patterns within complex processes. By training GNN models on historical process data, organizations can accurately forecast the duration of future processes based on their current state and past performance. This predictive capability allows businesses to proactively manage their workflows, optimize resource allocation, and anticipate potential delays, leading to more efficient and reliable operations.</p>"},{"location":"about/","title":"About the TigerGraph Process Mining Website","text":"<p>Unlock the Power of Your Processes</p> <p>In today's fast-paced business environment, understanding and optimizing your processes is crucial for staying competitive.</p> <p>TigerGraph customers know that graph data models are ideally suited to solving complex problems with event workflows. This website demonstrates how TigerGraph can be used a a core tool to solve complex process mining problems providing unparalleled insights into your business operations.</p> <p>Why Process Mining with TigerGraph?</p> <p>Process mining involves analyzing business processes based on event logs, helping organizations uncover inefficiencies, deviations, and opportunities for improvement. Traditional approaches often struggle with the complexity and interconnected nature of these processes. This is where TigerGraph excels.</p> <p>Key Benefits of Using TigerGraph for Process Mining:</p> <ol> <li> <p>Unrivaled Connectivity: Graph databases are inherently designed to manage and analyze interconnected data. TigerGraph's native graph architecture allows for the seamless exploration of complex relationships within your business processes, ensuring no detail is overlooked.</p> </li> <li> <p>Real-Time Analytics: With TigerGraph's high-performance capabilities, you can perform real-time analysis on your processes, enabling you to identify bottlenecks and inefficiencies as they happen, rather than after the fact.</p> </li> <li> <p>Scalability: TigerGraph's distributed architecture ensures that your process mining application can scale effortlessly, handling vast amounts of data without compromising on performance. This is especially critical as your organization grows and processes become more intricate.</p> </li> <li> <p>Complex Querying Made Simple: Process mining requires complex queries that span multiple entities and relationships. TigerGraph's powerful query language, GSQL, allows you to write sophisticated queries that can capture the nuances of your processes, providing deeper insights than traditional database systems.</p> </li> <li> <p>Enhanced Visualization: Visualizing processes and their interactions is a core part of process mining. TigerGraph integrates seamlessly with various visualization tools, enabling you to create dynamic and intuitive process maps that bring your data to life.</p> </li> </ol> <p>Explore Our Solutions</p> <p>Discover how TigerGraph can be leveraged to transform your approach to process mining. Our platform provides a comprehensive guide on implementing process mining applications using TigerGraph, complete with tutorials, case studies, and best practices.</p> <p>Join the Revolution</p> <p>Don't get left behind in the data-driven era. Explore how TigerGraph can empower your organization to achieve unparalleled process efficiency and insight. Start your journey with us today!</p>"},{"location":"contact/","title":"Contact","text":"<p>Dan McCreary LinkedIn</p>"},{"location":"glossary/","title":"Glossary of Terms for Process Mining","text":""},{"location":"glossary/#sample-prompt","title":"Sample Prompt","text":"<pre><code>You are an expert at creating precise definitions of technical terms using.  \nYour definitions are precise, concise, distinct and non-circular.  \nYou give a quick definition followed by an example.  \nYou always return the term name in a markdown level-4 \nheading followed by the definition and example.\n\nPlease create a list of business terms for an application program that does process mining.  \nMake sure the key terms are listed in alphabetical order.\n</code></pre>"},{"location":"glossary/#list-of-terms","title":"List of Terms","text":""},{"location":"glossary/#activity","title":"Activity","text":"<p>A specific task or action within a business process that contributes to the completion of that process. </p> <p>Example: In a purchase order process, activities might include \"Submit Purchase Request,\" \"Approve Request,\" and \"Issue Payment.\"</p>"},{"location":"glossary/#bottleneck","title":"Bottleneck","text":"<p>A point in a process where the flow of work is impeded, causing delays and reducing the overall efficiency of the process. </p> <p>In a hiring process, a bottleneck might occur during the interview scheduling phase if only a limited number of interviewers are available.</p>"},{"location":"glossary/#case","title":"Case","text":"<p>An individual instance of a business process, often represented by a unique identifier, tracking all activities and events related to that instance.</p> <p>Example: Each purchase order processed by a company can be considered a separate case, tracked from initiation to completion.</p>"},{"location":"glossary/#compliance","title":"Compliance","text":"<p>Adherence to laws, regulations, standards, or internal policies within a business process. </p> <p>Example: Ensuring that all financial transactions within a company are recorded and reported according to regulatory requirements is a matter of compliance.</p>"},{"location":"glossary/#conformance-checking","title":"Conformance Checking","text":"<p>The process of comparing the actual execution of a business process (as recorded in logs) against a predefined model to identify deviations or inefficiencies. </p> <p>Example: A conformance check might reveal that a step intended to be completed in three days took a week in actual practice, indicating a bottleneck.</p>"},{"location":"glossary/#data-cleaning","title":"Data Cleaning","text":"<p>The process of identifying and correcting errors, inconsistencies, or missing values in event data to ensure its accuracy and reliability for analysis. </p> <p>For example, data cleaning might involve removing duplicate entries or filling in missing timestamps in a log file.</p>"},{"location":"glossary/#data-extraction","title":"Data Extraction","text":"<p>The process of retrieving specific data from various sources, such as log files, to be used for analysis or further processing. </p> <p>For instance, data extraction might involve pulling timestamped events from a server log to analyze system performance over time.</p>"},{"location":"glossary/#data-integration","title":"Data Integration","text":"<p>The process of combining event data from multiple sources or systems into a cohesive dataset for process analysis. </p> <p>For example, data integration might involve merging customer service logs with sales transaction logs to analyze the end-to-end customer experience.#### Data Transformation</p> <p>The process of converting extracted raw event data into a structured format suitable for analysis, often involving normalization and aggregation. In a sales process, data transformation might involve standardizing date formats and combining multiple event logs into a unified dataset.</p>"},{"location":"glossary/#event","title":"Event","text":"<p>A record of a specific occurrence or action within a process, often timestamped and associated with a particular case. </p> <p>For example, the event \"Order Shipped\" marks the time when an order is dispatched from the warehouse in an order fulfillment process.</p>"},{"location":"glossary/#event-correlation","title":"Event Correlation","text":"<p>The process of linking related events across multiple log files or sources to provide a complete view of a process instance. </p> <p>In an IT monitoring process, event correlation might link an \"Error Logged\" event with a subsequent \"System Restarted\" event across different systems.</p>"},{"location":"glossary/#event-data","title":"Event Data","text":"<p>The raw information that records individual occurrences within a business process, typically stored in log files and used as the basis for process mining. </p> <p>In a customer service process, event data might include entries like \"Call Received,\" \"Ticket Created,\" and \"Issue Resolved.\"</p>"},{"location":"glossary/#event-filtering","title":"Event Filtering","text":"<p>The process of selecting specific events or types of events from a log file to focus on relevant data for analysis. </p> <p>For instance, event filtering might be used to isolate only the \"Payment Received\" events from a broader set of e-commerce transaction logs.</p>"},{"location":"glossary/#event-log","title":"Event Log","text":"<p>A chronological record of all events or activities associated with a business process, typically used as the primary data source in process mining. </p> <p>Example: An event log for an online order process might include timestamps for when the order was placed, confirmed, shipped, and delivered.</p>"},{"location":"glossary/#event-log-management","title":"Event Log Management","text":"<p>The process of collecting, storing, and organizing log files to ensure that event data is readily accessible for analysis. </p> <p>For example, event log management might involve setting up automated processes to archive older logs while maintaining access to recent logs for process mining.</p>"},{"location":"glossary/#key-performance-indicator-kpi","title":"Key Performance Indicator (KPI)","text":"<p>A measurable value that indicates how effectively a process or organization is achieving its objectives. </p> <p>For example, the average time to resolve customer support tickets is a KPI that reflects the efficiency of the support process.</p>"},{"location":"glossary/#lead-time","title":"Lead Time","text":"<p>The total time taken from the initiation of a process to its completion, including both active working time and any delays. </p> <p>In a manufacturing process, lead time includes the time from receiving an order to delivering the finished product to the customer.</p>"},{"location":"glossary/#log-parsing","title":"Log Parsing","text":"<p>The process of analyzing and interpreting log files to extract structured event data for further analysis. </p> <p>For example, log parsing might involve identifying and isolating key events such as \"Login Success\" or \"File Upload\" from a web server log.</p>"},{"location":"glossary/#process-discovery","title":"Process Discovery","text":"<p>The technique of automatically generating a business process model from event log data, revealing the actual workflow as executed.</p> <p>Very often the documented workflows do not follow the actual worklows.</p> <p>Example: By analyzing the event logs, process discovery might show that orders are often rerouted through additional approval steps not initially considered in the formal process.</p>"},{"location":"glossary/#process-instance","title":"Process Instance","text":"<p>A single execution of a business process, from start to finish, capturing all related activities and events. </p> <p>Each customer's journey through a loan application process represents a separate process instance.</p>"},{"location":"glossary/#process-mining","title":"Process Mining","text":"<p>A data-driven technique used to analyze and improve business processes by extracting insights from event logs. </p> <p>A company might use process mining to identify inefficiencies in their procurement process by analyzing the time taken at each step.</p>"},{"location":"glossary/#process-model","title":"Process Model","text":"<p>A formalized representation of a business process, often visual, showing the sequence of activities, decision points, and flow of information or materials. </p> <p>Example: A process model for handling customer complaints may include steps like \"Receive Complaint,\" \"Assess Issue,\" \"Assign to Representative,\" and \"Resolve Complaint.\"</p>"},{"location":"glossary/#process-optimization","title":"Process Optimization","text":"<p>The practice of making changes to a process to improve its efficiency, effectiveness, or adaptability. </p> <p>For instance, automating certain repetitive tasks in a payroll process can be a form of process optimization that reduces processing time.</p>"},{"location":"glossary/#resource","title":"Resource","text":"<p>Any entity (person, machine, system) that performs or supports activities within a business process. </p> <p>Example: In a manufacturing process, resources might include operators, assembly line robots, and quality control software.</p>"},{"location":"glossary/#root-cause-analysis","title":"Root Cause Analysis","text":"<p>A method used to identify the fundamental reason for a problem or defect within a process. If customer complaints about late deliveries are increasing, root cause analysis might reveal that delays in the supply chain are the primary issue.</p>"},{"location":"glossary/#throughput-time","title":"Throughput Time","text":"<p>The total time taken from the start to the end of a process or case, encompassing all included activities and delays. </p> <p>Example: If a customer service ticket is opened on Monday and resolved on Wednesday, the throughput time for that ticket is two days.</p>"},{"location":"glossary/#timestamp","title":"Timestamp","text":"<p>A specific date and time recorded for each event within a log file, used to sequence and analyze the flow of activities within a process. In a shipping process, a timestamp might record the exact moment a package was dispatched from the warehouse.</p>"},{"location":"glossary/#variant","title":"Variant","text":"<p>A unique sequence of activities that occurs in a business process, representing a specific way in which the process is executed. For instance, in an insurance claim process, one variant might involve direct approval, while another requires further investigation before approval.</p>"},{"location":"references/","title":"Process Mining References","text":""},{"location":"references/#standards-for-process-mining","title":"Standards for Process Mining","text":"<p>The W3C Model for Provenance of Data This model is where we being. We model what Agent did what Activity on what Entity.</p>"},{"location":"references/#research-papers","title":"Research Papers","text":"<p>Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities - July 11, 2023</p>"},{"location":"use-cases/","title":"Process Mining Use Cases","text":"<p>Here are several examples of how process mining can be used to improve business processes across various domains:</p>"},{"location":"use-cases/#top-process-mining-use-cases","title":"Top Process Mining Use Cases","text":""},{"location":"use-cases/#optimizing-user-interface-design","title":"Optimizing User Interface Design","text":"<p>Process Mining Application: By analyzing event logs from user interactions with a software application, process mining can reveal which UI elements are frequently interacted with and which are ignored. This data can be used to streamline the interface, reduce clutter, and make critical features more accessible. </p> <p>Example: A company notices that users frequently abandon a form halfway through. Process mining shows that a specific field is often left blank, leading users to exit. Redesigning the form to clarify this field or making it optional improves completion rates.</p>"},{"location":"use-cases/#reducing-call-center-wait-times","title":"Reducing Call Center Wait Times","text":"<p>Process mining can be used to analyze the flow of calls through a call center, identifying bottlenecks where calls are delayed or transferred multiple times before resolution. By pinpointing these issues, the process can be adjusted to reduce wait times and improve customer satisfaction. </p> <p>Example: A call center analysis reveals that calls are frequently transferred between departments, leading to long wait times. By reorganizing the call routing process based on the analysis, the company reduces transfers and shortens call resolution times.</p>"},{"location":"use-cases/#enhancing-web-commerce-checkout-processes","title":"Enhancing Web Commerce Checkout Processes","text":"<p>In web commerce, process mining can track the customer journey through the checkout process, identifying steps where customers frequently drop off or abandon their carts. This insight allows businesses to streamline the checkout process and reduce cart abandonment. </p> <p>Example: An online retailer discovers that many customers abandon their carts at the payment step. Process mining shows that customers are hesitant to enter credit card information without seeing a final total with shipping costs. Adding a summary page before payment reduces abandonment rates.</p>"},{"location":"use-cases/#improving-customer-service-ticket-resolution","title":"Improving Customer Service Ticket Resolution","text":"<p>By analyzing the event logs of customer service tickets, process mining can identify stages in the resolution process where tickets are often delayed. This information can be used to optimize workflows and ensure faster resolutions. </p> <p>Example: A helpdesk process mining analysis reveals that tickets often stall when escalated to a particular team. By reallocating resources or providing additional training to that team, the company can reduce the average time to resolve tickets.</p>"},{"location":"use-cases/#optimizing-sales-funnel-sales-pipeline","title":"Optimizing Sales Funnel (Sales Pipeline)","text":"<p>Process mining can analyze the flow of potential customers through a sales funnel, identifying where prospects drop off and why. This data can be used to refine marketing strategies and optimize conversion rates.</p> <p>Example: A SaaS company uses process mining to track prospects from initial contact through to conversion. The analysis shows that a large number of prospects drop off after the trial period ends. Offering a discount or additional support during this period improves conversions.</p> <p>Detailed Example of Stages and KPIs</p>"},{"location":"use-cases/#streamlining-user-onboarding","title":"Streamlining User Onboarding","text":"<p>Process mining can analyze the steps new users take when onboarding to a digital service, identifying common pitfalls or steps where users struggle. The onboarding process can then be optimized to make it smoother and more intuitive. </p> <p>Example: A SaaS company finds that users often get stuck at a particular tutorial step during onboarding. By revising the tutorial to provide clearer instructions or automating certain steps, the company reduces churn and increases user activation rates.</p>"},{"location":"use-cases/#reducing-cart-abandonment-in-e-commerce","title":"Reducing Cart Abandonment in E-commerce","text":"<p>By tracking the flow of customers through an e-commerce site, process mining can identify common points where users abandon their shopping carts. This insight helps businesses to redesign the checkout process to retain more customers. </p> <p>Example: Process mining reveals that a significant drop-off occurs when customers are asked to create an account before checking out. Implementing a guest checkout option based on this insight helps retain customers and reduce abandonment.</p>"},{"location":"use-cases/#enhancing-user-experience-in-mobile-apps","title":"Enhancing User Experience in Mobile Apps","text":"<p>Process mining can track user interactions within a mobile app, identifying common pain points or features that are underutilized. This information can guide redesign efforts to improve the user experience. </p> <p>Example: A fitness app discovers through process mining that users rarely complete workout plans. The analysis shows that users often get frustrated with setting up custom workouts. By simplifying the workout creation process, the app increases user engagement and retention.</p>"},{"location":"use-cases/#improving-first-call-resolution-in-call-centers","title":"Improving First Call Resolution in Call Centers","text":"<p>Process mining can be used to analyze the sequence of steps taken during customer support calls, identifying patterns that lead to first call resolution (FCR) or repeated calls. This insight allows call centers to train agents on the most effective resolution strategies. </p> <p>Example: A call center finds that customers often have to call back due to incomplete issue resolution. Process mining shows that calls handled by agents following a certain script have higher FCR rates. Standardizing this script across the center improves overall efficiency.</p>"},{"location":"use-cases/#reducing-response-times-in-customer-service","title":"Reducing Response Times in Customer Service","text":"<p>Process mining can analyze the workflow of customer service teams, identifying steps in the process where response times are longer than average. This insight allows for the optimization of workflows and better resource allocation. </p> <p>Example: A customer service department notices that response times are longest during the ticket reassignment phase. Process mining identifies that tickets are often reassigned due to incomplete information. By improving the initial information-gathering process, the company reduces overall response times.</p> <p>These examples demonstrate the versatility of process mining in improving various aspects of business processes, leading to increased efficiency, better customer experiences, and higher conversion rates.</p>"},{"location":"use-cases/#more-unusual-forms-of-process-mining","title":"More Unusual Forms of Process Mining","text":""},{"location":"use-cases/#analyzing-data-lineage-in-etl-processes","title":"Analyzing Data Lineage in ETL Processes","text":"<p>Process mining can be applied to track the lineage of data through complex Extract, Transform, Load (ETL) processes, revealing how data is transformed at each step and identifying potential errors or inefficiencies in data handling. </p> <p>Example: A financial institution uses process mining to trace the flow of data from raw transaction logs through various transformations and aggregations before being loaded into a reporting database. This analysis helps identify a step where data accuracy is compromised, leading to more reliable financial reports.</p>"},{"location":"use-cases/#monitoring-employee-movement-in-smart-offices","title":"Monitoring Employee Movement in Smart Offices","text":"<p>Process mining can be used to analyze data from smart office sensors to understand patterns in employee movement and interaction, optimizing office layouts and improving productivity. </p> <p>Example: A company uses process mining to track how employees move between workstations, meeting rooms, and common areas. The analysis reveals that certain paths are overused, leading to congestion. The company rearranges furniture and workspaces to improve flow and reduce interruptions.</p>"},{"location":"use-cases/#optimizing-food-supply-chains-for-freshness","title":"Optimizing Food Supply Chains for Freshness","text":"<p>In the food industry, process mining can be applied to track the journey of perishable goods through the supply chain, ensuring that freshness is maintained from farm to table. </p> <p>Example: A grocery chain uses process mining to analyze the flow of produce from suppliers to stores. The analysis shows that certain distribution centers introduce delays that reduce freshness. By rerouting deliveries, the chain ensures fresher produce reaches customers.</p>"},{"location":"use-cases/#detecting-anomalies-in-scientific-research-data-provenance","title":"Detecting Anomalies in Scientific Research Data Provenance","text":"<p>Process Mining Application: Process mining can track the provenance of scientific research data, ensuring that data is collected, processed, and analyzed according to rigorous standards, and identifying any deviations that might affect the integrity of the results. Example: A research institution applies process mining to track the flow of experimental data from collection through analysis. The analysis reveals a pattern where data from one set of experiments consistently deviates from the expected processing steps, prompting a review that uncovers a calibration error in the equipment.</p>"},{"location":"use-cases/#analyzing-patient-flow-in-hospitals-to-reduce-infection-risk","title":"Analyzing Patient Flow in Hospitals to Reduce Infection Risk","text":"<p>Process mining can analyze the movement of patients, staff, and equipment within a hospital to identify patterns that may increase the risk of infection spread, leading to improved protocols and safer environments. </p> <p>Example: A hospital uses process mining to track the movement of patients and staff in an ICU. The analysis identifies certain corridors and rooms that are frequently used by both infectious and non-infectious patients, prompting changes to reduce cross-contamination risks.</p>"},{"location":"use-cases/#enhancing-the-auditability-of-ai-decision-making-processes","title":"Enhancing the Auditability of AI Decision-Making Processes","text":"<p>Process Mining Application: Process mining can be used to trace the decision-making process of AI systems, providing a clear audit trail of how inputs are processed and decisions are made, which is crucial for regulatory compliance and trust. Example: A financial services company uses process mining to audit the decision-making process of an AI system used for loan approvals. The analysis reveals that certain data points are weighted more heavily than intended, leading to adjustments that make the system's decisions more transparent and fair.</p>"},{"location":"use-cases/#tracing-the-evolution-of-legal-documents-in-contract-management","title":"Tracing the Evolution of Legal Documents in Contract Management","text":"<p>In legal contract management, process mining can be used to trace the evolution of contracts and other legal documents, providing insights into how they have been modified over time and ensuring that all changes are tracked and justified. </p> <p>Example: A law firm applies process mining to analyze the revision history of complex contracts. The analysis shows that certain clauses have been consistently modified in ways that introduce legal risks. The firm uses this insight to standardize language and reduce the likelihood of future disputes.</p>"},{"location":"use-cases/#optimizing-the-flow-of-materials-in-manufacturing-recycling-loops","title":"Optimizing the Flow of Materials in Manufacturing Recycling Loops","text":"<p>In manufacturing, process mining can be applied to optimize the flow of materials through recycling loops, ensuring that waste is minimized and resources are efficiently reused. </p> <p>Example: An electronics manufacturer uses process mining to analyze the flow of scrap materials through its recycling process. The analysis identifies stages where valuable materials are not fully recovered, leading to changes that improve recycling efficiency and reduce costs.</p>"},{"location":"use-cases/#analyzing-knowledge-transfer-in-corporate-training-programs","title":"Analyzing Knowledge Transfer in Corporate Training Programs","text":"<p>Process mining can be used to analyze how knowledge is transferred within an organization during training programs, identifying bottlenecks in learning and opportunities to enhance the training process. </p> <p>Example: A large corporation uses process mining to track the progress of employees through a new training program. The analysis shows that certain modules are consistently difficult for employees to complete, leading to targeted improvements that enhance learning outcomes.</p>"},{"location":"use-cases/#tracing-the-history-of-artistic-collaborations-in-digital-media-projects","title":"Tracing the History of Artistic Collaborations in Digital Media Projects","text":"<p>In the creative industries, process mining can be applied to trace the collaboration history in digital media projects, helping to understand how different artists and designers contribute to the final product. </p> <p>Example: A digital media studio uses process mining to analyze the collaboration history of a complex video project. The analysis reveals that certain artists contribute more during the early stages of the project, leading to a reallocation of resources to balance workloads and improve efficiency.</p>"},{"location":"why-graph/","title":"Why Graph for Process Mining?","text":"<p>Here's a detailed list of why graph databases are ideal for working with process mining problems, particularly for the creation and analysis of complex workflows:</p>"},{"location":"why-graph/#1-natural-representation-of-workflows-as-graphs","title":"1. Natural Representation of Workflows as Graphs","text":"<p>Graph databases inherently model relationships and connections, making them a natural fit for representing complex workflows. Each activity in a process can be modeled as a node, and the transitions or dependencies between activities can be represented as edges. This direct mapping simplifies both the storage and querying of process data.</p> <p>Example: In a manufacturing workflow, each production step is a node, and the dependencies between these steps (e.g., Step B can't start until Step A is completed) are edges, easily modeled and analyzed in a graph database.</p>"},{"location":"why-graph/#2-efficient-handling-of-complex-relationships","title":"2. Efficient Handling of Complex Relationships","text":"<p>Process mining often involves analyzing complex, interconnected relationships between various activities, cases, and resources. Graph databases excel at storing and querying these intricate relationships, allowing for more efficient analysis of how different parts of a process interact.</p> <p>Example: Analyzing how different teams interact within a large organization's project management process can be efficiently managed with a graph database, where each team, task, and interaction is a node, and relationships between them are edges.</p>"},{"location":"why-graph/#3-dynamic-and-flexible-schema","title":"3. Dynamic and Flexible Schema","text":"<p>Process mining requires dealing with dynamic processes that evolve over time. Graph databases offer schema flexibility, allowing changes in the process structure to be easily accommodated without the need for complex migrations.</p> <p>Example: If a new approval step is added to a business process, the graph schema can be easily adjusted to include this step without requiring major changes to the existing data structure.</p>"},{"location":"why-graph/#4-advanced-pathfinding-capabilities","title":"4. Advanced Pathfinding Capabilities","text":"<p>Graph databases are optimized for traversing paths, which is critical for process mining tasks such as finding the most efficient sequence of activities or identifying bottlenecks. Algorithms like Dijkstra's or A* can be directly applied to explore optimal paths within the process graph.</p> <p>Example: In an order fulfillment process, finding the shortest or fastest path from order placement to delivery can be quickly computed, highlighting areas where delays occur.</p>"},{"location":"why-graph/#5-graph-query-languages-eg-gsql-cypher","title":"5. Graph Query Languages (e.g., GSQL, Cypher)","text":"<p>Graph query languages are designed to easily express complex queries that involve traversals, pattern matching, and recursive relationships. This makes it easier to perform sophisticated process mining queries, such as identifying frequent patterns or deviations in workflows.</p> <p>Example: Using GSQL or Cypher, a query can be crafted to find all instances where a process deviates from the standard workflow, such as skipping an approval step.</p>"},{"location":"why-graph/#6-visualization-of-process-flows","title":"6. Visualization of Process Flows","text":"<p>Graph databases often integrate well with visualization tools that can directly map the graph structure to visual representations, making it easier to understand and communicate complex process flows.</p> <p>Example: A visual graph representation of a customer service process can clearly show the various pathways a customer inquiry might take, highlighting where delays or frequent re-routing occurs.</p>"},{"location":"why-graph/#7-real-time-process-monitoring","title":"7. Real-Time Process Monitoring","text":"<p>Graph databases can support real-time data updates and queries, which is crucial for monitoring ongoing processes and making timely decisions. This is particularly important in environments where processes need to be adjusted on-the-fly based on current data.</p> <p>Example: In a logistics operation, real-time monitoring of delivery routes and times can be managed with a graph database, allowing for immediate adjustments in case of delays or disruptions.</p>"},{"location":"why-graph/#8-integration-with-machine-learning-and-ai","title":"8. Integration with Machine Learning and AI","text":"<p>Graph databases can easily integrate with machine learning and AI frameworks, enabling advanced analysis like predictive process monitoring, anomaly detection, and process optimization. The ability to store and analyze graph embeddings also enhances predictive modeling.</p> <p>Example: A graph-based model could predict potential bottlenecks in a production process by analyzing historical data and current process states, providing actionable insights to prevent delays.</p>"},{"location":"why-graph/#9-richness-in-representing-multi-dimensional-data","title":"9. Richness in Representing Multi-Dimensional Data","text":"<p>Graph databases allow for the inclusion of rich metadata and multi-dimensional data at each node and edge, enabling a more nuanced analysis of processes that include factors like cost, time, resource usage, and compliance.</p> <p>Example: In a healthcare process, nodes can represent different treatment stages, while edges can carry information about time taken, costs involved, and patient outcomes, allowing for a comprehensive analysis of treatment effectiveness.</p>"},{"location":"why-graph/#10-effective-anomaly-detection","title":"10. Effective Anomaly Detection","text":"<p>Anomalies in process execution, such as unexpected sequences of events or deviations from the norm, can be more easily detected in a graph database where complex relationships and dependencies are explicitly modeled and analyzed.</p> <p>Example: An anomaly detection algorithm can be run on a graph representing a financial auditing process to identify any unusual transactions that deviate from standard practices.</p>"},{"location":"why-graph/#11-support-for-hierarchical-and-multi-level-processes","title":"11. Support for Hierarchical and Multi-Level Processes","text":"<p>Many business processes are hierarchical, with sub-processes nested within larger processes. Graph databases naturally support hierarchical data structures, enabling detailed analysis of both macro-level workflows and micro-level sub-processes.</p> <p>Example: A supply chain process might be modeled with high-level nodes representing entire stages (e.g., procurement, production, distribution), with sub-nodes detailing individual activities within each stage.</p>"},{"location":"why-graph/#12-effective-time-based-analysis","title":"12. Effective Time-Based Analysis","text":"<p>Graph databases can efficiently model time-based relationships, allowing for the analysis of how processes evolve over time, including the identification of trends, seasonal patterns, and the impact of time on process efficiency.</p> <p>Example: Analyzing a graph-based model of a retail sales process can reveal how certain time periods, like holidays, affect the speed and efficiency of order processing.</p>"},{"location":"why-graph/#13-cross-process-analysis","title":"13. Cross-Process Analysis","text":"<p>Graph databases enable the correlation and analysis of multiple processes that share common elements, facilitating a holistic view of how different business processes interact and influence each other.</p> <p>Example: In a large enterprise, process mining across different departments (e.g., sales, support, finance) can identify interdependencies and optimize cross-departmental workflows.</p>"},{"location":"why-graph/#14-scalability-for-large-scale-process-data","title":"14. Scalability for Large-Scale Process Data","text":"<p>Graph databases are designed to scale horizontally, making them ideal for handling the massive amounts of event log data typically generated by large organizations, without compromising on performance.</p> <p>Example: A multinational corporation can use a graph database to manage and analyze millions of process instances across different regions, ensuring that global process standards are met while allowing for local variations.</p>"},{"location":"why-graph/#15-enhanced-compliance-and-auditability","title":"15. Enhanced Compliance and Auditability","text":"<p>Graph databases provide a clear, traceable record of how processes are executed, which is essential for compliance monitoring and auditing. The ability to trace each step in a process back to its origin helps ensure that all regulatory requirements are met.</p> <p>Example: A financial institution can use a graph database to audit loan approval processes, ensuring that every step is compliant with regulatory standards and that no steps are skipped or altered.</p>"},{"location":"why-graph/#16-support-for-complex-event-processing-cep","title":"16. Support for Complex Event Processing (CEP)","text":"<p>Graph databases can integrate with CEP systems to analyze and react to streams of events in real-time, allowing for immediate process adjustments based on the current state of the workflow.</p> <p>Example: In a network security process, a graph database might work with a CEP system to detect and respond to patterns of events that indicate a potential security breach, triggering automatic containment measures.</p> <p>These points highlight how graph databases provide a robust, flexible, and efficient platform for modeling, analyzing, and optimizing complex workflows in process mining, offering capabilities that are difficult to achieve with traditional relational databases.</p>"},{"location":"concepts/","title":"Process Mining Concepts","text":""},{"location":"concepts/#introduction","title":"Introduction","text":"<p>Introduction</p>"},{"location":"concepts/#event-logs","title":"Event Logs","text":"<p>Event Logs</p>"},{"location":"concepts/#sales-pipeline","title":"Sales Pipeline","text":"<p>Sales Pipeline</p>"},{"location":"concepts/#gnns","title":"GNNs","text":"<p>Use of GNN</p>"},{"location":"concepts/event-logs/","title":"Event Logs","text":"<p>Gathering precise data from event logs is the first step in process mining.</p>"},{"location":"concepts/event-logs/#introduction","title":"Introduction","text":"<p>Event logs are chronological records of events that capture the activities occurring within a system, process, or application. Each entry in an event log typically contains specific information such as the event name, timestamp, associated entities (like users or processes), and possibly additional metadata like status codes or resource identifiers. Event logs serve as a foundational data source for process mining, providing insights into how processes are executed, where bottlenecks occur, and how different events relate to each other.</p>"},{"location":"concepts/event-logs/#types-of-event-logs","title":"Types of Event Logs","text":"<p>Event logs can vary significantly depending on the context in which they are generated. Here are some common types:</p>"},{"location":"concepts/event-logs/#system-event-logs","title":"System Event Logs","text":"<p>These logs are generated by operating systems or server applications and record events related to system activities like user logins, file access, or system errors.</p> <p>Example: A Windows event log might capture events such as \"User Login,\" \"File Created,\" or \"System Shutdown.\"</p>"},{"location":"concepts/event-logs/#application-event-logs","title":"Application Event Logs","text":"<p>Generated by software applications, these logs capture events specific to the application's operation, such as user actions, errors, or transactions.</p> <p>Example: A web application log might record events like \"User Registered,\" \"Order Placed,\" or \"Error 404: Page Not Found.\"</p>"},{"location":"concepts/event-logs/#business-process-event-logs","title":"Business Process Event Logs","text":"<p>These logs record events related to specific business processes, tracking the flow of activities from start to finish.</p> <p>Example: An ERP system might generate logs for a purchase order process, recording events like \"Order Created,\" \"Order Approved,\" and \"Payment Processed.\"</p>"},{"location":"concepts/event-logs/#security-event-logs","title":"Security Event Logs","text":"<p>Focused on security-related events, these logs capture incidents like login attempts, access control violations, and security alerts.</p> <p>Example: A security log might include events such as \"Failed Login Attempt,\" \"Unauthorized Access Attempt,\" or \"Firewall Breach Detected.\"</p>"},{"location":"concepts/event-logs/#iot-event-logs","title":"IoT Event Logs","text":"<p>Generated by Internet of Things (IoT) devices, these logs capture data from sensors, actuators, and other connected devices.</p> <p>Example: A smart home system might generate logs for events like \"Temperature Sensor Activated,\" \"Door Opened,\" or \"Motion Detected.\"</p>"},{"location":"concepts/event-logs/#step-by-step-process-for-analyzing-event-logs","title":"Step-by-Step Process for Analyzing Event Logs","text":"<p>Analyzing event logs involves several steps, each aimed at extracting meaningful insights from raw event data.  Here's a detailed overview of the process:</p>"},{"location":"concepts/event-logs/#step-1-data-collection","title":"Step 1: Data Collection","text":"<p>Objective: Gather event logs from various sources such as servers, applications, IoT devices, or business systems.</p> <p>Process: -   Identify and connect to the relevant log sources. -   Extract logs using automated tools or manual processes. -   Ensure that logs are collected in a standardized format to facilitate analysis.</p>"},{"location":"concepts/event-logs/#step-2-data-preprocessing","title":"Step 2: Data Preprocessing","text":"<p>Objective: Clean, normalize, and prepare the event data for analysis. Process: -   Log Parsing: Convert raw log entries into a structured format (e.g., tables or JSON objects). -   Data Cleaning: Remove duplicates, filter irrelevant events, and correct any obvious errors in the data. -   Timestamp Normalization: Ensure all events are synchronized to a common time format, especially when logs come from multiple time zones or systems. -   Event Correlation: Link related events across different logs to form a cohesive view of the process (e.g., correlating a \"User Login\" event with a \"File Access\" event).</p>"},{"location":"concepts/event-logs/#step-3-process-discovery","title":"Step 3: Process Discovery","text":"<p>Objective: Generate a process model that represents the flow of activities as captured by the event logs. Process: -   Sequential Ordering: Arrange events in chronological order for each case or instance. -   Pattern Detection: Identify recurring sequences of events that represent typical workflows. -   Model Construction: Use process mining tools to automatically construct a visual process model, such as a flowchart or Petri net, from the event sequences.</p>"},{"location":"concepts/event-logs/#step-4-conformance-checking","title":"Step 4:  Conformance Checking","text":"<p>Objective: Compare the discovered process model with an ideal or predefined model to identify deviations. Process: -   Alignment: Map the events from the log to the corresponding steps in the predefined model. -   Deviation Detection: Highlight any discrepancies where the actual process deviates from the expected workflow, such as skipped steps or additional activities. -   Root Cause Analysis: Investigate the reasons for deviations, such as process bottlenecks, errors, or exceptional cases.</p>"},{"location":"concepts/event-logs/#step-5-performance-analysis","title":"Step 5: Performance Analysis","text":"<p>Objective: Analyze the performance of the process by examining metrics like throughput time, resource utilization, and bottlenecks.</p> <p>Process: -   Time Analysis: Calculate the time taken for each process instance, activity, and transition. -   Bottleneck Identification: Identify stages where processes are delayed, resources are under or over-utilized, or there is a high rate of rework. -   Optimization Opportunities: Highlight areas where process improvements can be made, such as by automating repetitive tasks or reallocating resources.</p>"},{"location":"concepts/event-logs/#step-6-anomaly-detection","title":"Step 6: Anomaly Detection","text":"<p>Objective: Identify unusual patterns or events that deviate significantly from the norm, indicating potential issues or opportunities.</p> <p>Process:     -   Statistical Analysis: Use statistical methods to detect outliers in event frequency, sequence, or timing.     -   Machine Learning: Apply machine learning models to predict and flag anomalies based on historical event log data.     -   Investigation: Analyze flagged anomalies to determine their cause and potential impact on the overall process.</p>"},{"location":"concepts/event-logs/#challenges-of-incomplete-event-logs","title":"Challenges of Incomplete Event Logs","text":"<p>Many organizations make detailed logging a centerpiece of their new developer training. They require all developers to take a course on event logging and hold third party applications responsible by contract to support custom logging features when needed.</p> <p>Unfortunalty, many organizations don't take these steps to train their staff and hold third party application responsible.  The result is poor quality logging and the inability to change the grain of logging detail.</p> <p>Missing data in event logs can pose significant challenges to process mining and analysis. Missing events can lead to incomplete or inaccurate process models, flawed conformance checks, and misleading performance metrics. Here's a discussion of common problems associated with missing data and strategies to overcome these challenges:</p>"},{"location":"concepts/event-logs/#problems-due-to-missing-data","title":"Problems Due to Missing Data","text":"<ol> <li>Incomplete Logs: Missing events can result in process models that do not fully capture the actual workflow, leading to gaps in understanding.</li> <li>Missing Data in Logs: Events don't associate the event with the correct user or agent ID for consistent tracking across log files.</li> <li>Skewed Performance Metrics: If critical timestamps are missing, calculations like throughput time or cycle time may be inaccurate, skewing performance analysis.</li> <li>Inaccurate Conformance Checking: Missing events can cause the actual process to appear more conformant to the ideal model than it actually is, leading to false conclusions.</li> <li>Lack of Logging Level Standards: </li> <li>Challenges in Anomaly Detection: Anomalies might go undetected if key data points are missing, reducing the effectiveness of anomaly detection efforts.</li> </ol>"},{"location":"concepts/event-logs/#2-strategies-to-overcome-missing-data-challenges","title":"2. Strategies to Overcome Missing Data Challenges","text":"<ul> <li>Data Imputation: Use statistical methods or machine learning models to estimate and fill in missing data points based on available information. For instance, if a timestamp is missing, it might be estimated based on the average time between the previous and next events.</li> <li>Process Flexibility: Design process models that can handle optional or missing steps. This approach acknowledges that some events might not always occur but allows the model to remain valid and useful.</li> <li>Error Logging and Alerts: Implement robust logging mechanisms that detect and flag missing or incomplete data at the point of data collection, triggering alerts for immediate investigation.</li> <li>Data Redundancy: Collect data from multiple sources or systems to ensure that if one source is missing events, the others can provide the needed information. For example, cross-referencing system logs with user activity logs might fill gaps.</li> <li>User Feedback Loops: Implement feedback loops where users can manually confirm or input missing data, ensuring that the most critical data points are captured accurately.</li> <li>Incremental Data Collection: Revisit and collect additional data at later stages if missing information is discovered, ensuring that the event logs remain as complete as possible over time.</li> </ul>"},{"location":"concepts/event-logs/#conclusion","title":"Conclusion","text":"<p>Event logs are crucial for understanding and analyzing business processes, providing detailed records of activities that can be mined for insights. However, analyzing event logs requires careful preprocessing, accurate modeling, and robust strategies to address challenges like missing data. By applying these methods, organizations can use event logs to improve process efficiency, ensure compliance, and drive informed decision-making.</p>"},{"location":"concepts/foundational-models/","title":"Foundational Models for Process Mining","text":""},{"location":"concepts/foundational-models/#w3c-standards","title":"W3C Standards","text":"<p>In 2013 the experts at the World-</p>"},{"location":"concepts/gnn/","title":"Use of GNNs to Predict Future Events","text":"<p>Graph Neural Networks (GNNs) can be used to predict future events in a graph where the vertices represent events, and the edges (e.g., <code>NEXT_EVENT</code> and <code>PREV_EVENT</code>) contain information such as duration. Here's how this can be done:</p>"},{"location":"concepts/gnn/#1-graph-representation","title":"1. Graph Representation:","text":"<ul> <li>Vertices: Each vertex in the graph represents an event.</li> <li>Edges: The edges represent the relationship between events, such as which event follows another (<code>NEXT_EVENT</code>) or precedes another (<code>PREV_EVENT</code>). These edges contain a <code>duration</code> attribute that indicates the time between the events.</li> </ul>"},{"location":"concepts/gnn/#2-feature-representation","title":"2. Feature Representation:","text":"<ul> <li>Vertex Features: Each event vertex can have features like event type, timestamp, associated data, etc.</li> <li>Edge Features: The edges contain the duration, which is a crucial feature for predicting the timing of future events.</li> </ul>"},{"location":"concepts/gnn/#3-graph-construction-for-gnn","title":"3. Graph Construction for GNN:","text":"<ul> <li>The graph is constructed where the event vertices are connected via directed edges (<code>NEXT_EVENT</code>/<code>PREV_EVENT</code>) with the duration as a feature of the edge.</li> <li>This graph is then fed into a GNN, where the model can learn the temporal and sequential dependencies between events.</li> </ul>"},{"location":"concepts/gnn/#4-gnn-model-architecture","title":"4. GNN Model Architecture:","text":"<ul> <li>Input Layer: The input layer processes the features of the event vertices and the edge features (durations).</li> <li>Graph Convolutional Layers: These layers propagate information between connected vertices. For instance, a message-passing mechanism can aggregate information from neighboring events (previous or next) to update the representation of the current event vertex.</li> <li>Temporal Embedding: The duration feature can be explicitly used to create temporal embeddings that allow the model to learn the time-based dependencies between events.</li> <li>Recurrent Layer (optional): In some cases, a recurrent layer (like an LSTM or GRU) can be used to model the sequential nature of events further.</li> </ul>"},{"location":"concepts/gnn/#5-training-the-gnn","title":"5. Training the GNN:","text":"<ul> <li>Loss Function: The loss function is typically designed to minimize the difference between predicted and actual future event times or to classify the type of future event correctly.</li> <li>Supervision: The model can be supervised by using historical event sequences where the ground truth for future events is known.</li> </ul>"},{"location":"concepts/gnn/#6-prediction-process","title":"6. Prediction Process:","text":"<ul> <li>Prediction: For a given current graph of events, the trained GNN can predict the next event(s) by considering the learned relationships (from the graph structure and edge durations) and output a prediction for the next event's occurrence time or the type of event that will happen next.</li> <li>Inference: During inference, you can use the GNN to predict not just the immediate next event but potentially a sequence of future events by iteratively updating the graph with predicted events and re-applying the model.</li> </ul>"},{"location":"concepts/gnn/#7-handling-duration-in-prediction","title":"7. Handling Duration in Prediction:","text":"<ul> <li>The duration between events is key to predicting future events. The GNN uses the duration in the edge features to inform predictions of the timing of future events.</li> <li>By learning from the historical duration data, the model can predict how long until the next event occurs.</li> </ul>"},{"location":"concepts/gnn/#8-example-use-case","title":"8. Example Use Case:","text":"<ul> <li>Suppose you have a graph where each vertex represents a customer order event, and edges represent the sequence of orders, with the duration representing the time between orders.</li> <li>The GNN can learn to predict when the next order will happen based on the past sequence of orders and durations between them.</li> </ul>"},{"location":"concepts/gnn/#9-potential-gnn-models","title":"9. Potential GNN Models:","text":"<ul> <li>Graph Convolutional Networks (GCN): To learn node embeddings considering the local structure.</li> <li>Graph Attention Networks (GAT): To learn which events (nodes) are more influential when predicting the next event.</li> <li>Temporal GNNs: Specifically designed to handle temporal information and predict future events.</li> </ul>"},{"location":"concepts/gnn/#10-implementation-tips","title":"10. Implementation Tips:","text":"<ul> <li>Data Preparation: Ensure that your graph data is prepared with the correct vertex and edge features, especially focusing on accurate duration data.</li> <li>Model Tuning: Experiment with different GNN architectures and hyperparameters, such as the number of layers, embedding size, and learning rate, to optimize performance.</li> <li>Temporal Information: Consider incorporating additional temporal features, such as time of day or day of the week, which could improve predictions.</li> </ul> <p>By applying a GNN in this manner, you can effectively model and predict future events in a graph, leveraging the sequential and temporal relationships embedded in the graph's structure and edge features.</p>"},{"location":"concepts/introduction/","title":"Introduction to Process Mining Concepts","text":""},{"location":"concepts/introduction/#step-1-event-logs","title":"Step 1: Event Logs","text":"<p>The foundational data source in process mining, event logs capture the sequence of events within a process, including timestamps, case IDs, and event types. The first step in any process mining effort is to gather and prepare these logs for analysis.</p>"},{"location":"concepts/introduction/#step-2-case","title":"Step 2: Case","text":"<p>A case represents a single instance of the process being analyzed. In event logs, cases are identified by unique IDs and are essential for grouping related events together to understand the sequence of activities for each instance.</p>"},{"location":"concepts/introduction/#step-3-activity","title":"Step 3: Activity","text":"<p>An activity is a specific task or action within a process. Activities are the building blocks of process models and are derived from the events recorded in the event logs.</p>"},{"location":"concepts/introduction/#step-4-timestamp","title":"Step 4: Timestamp","text":"<p>Timestamps are the recorded times at which events occur. They are crucial for sequencing events within a case, calculating durations, and analyzing the timing and order of activities in the process.</p>"},{"location":"concepts/introduction/#step-5-process-discovery","title":"Step 5: Process Discovery","text":"<p>Process discovery involves automatically generating a visual or structured representation (model) of the process from the event logs. This step reveals the actual process flow as it happens in practice, which might differ from the intended process.</p>"},{"location":"concepts/introduction/#step-6-conformance-checking","title":"Step 6: Conformance Checking","text":"<p>Conformance checking compares the discovered process model against a predefined (or ideal) process model. This step identifies deviations, inefficiencies, and non-compliant behavior within the process.</p>"},{"location":"concepts/introduction/#step-7-performance-analysis","title":"Step 7: Performance Analysis","text":"<p>This step involves analyzing the timing and efficiency of the process, using metrics like throughput time, waiting times, and bottleneck identification. Performance analysis helps quantify how well the process operates.</p>"},{"location":"concepts/introduction/#step-8-variant-analysis","title":"Step 8: Variant Analysis","text":"<p>Variant analysis identifies different ways (variants) in which a process is executed. It compares these variants to understand why certain cases follow different paths and what impact these variations have on process outcomes.</p>"},{"location":"concepts/introduction/#step-9-root-cause-analysis","title":"Step 9: Root Cause Analysis","text":"<p>Once deviations or inefficiencies are identified, root cause analysis is used to understand the underlying reasons for these issues. This analysis can involve looking at specific cases, activities, or conditions that lead to process problems.</p>"},{"location":"concepts/introduction/#step-10-process-optimization","title":"Step 10: Process Optimization","text":"<p>The final step in process mining involves using insights gained from the previous steps to optimize the process. This can include redesigning the process, automating tasks, reallocating resources, or implementing new controls to improve overall efficiency and compliance.</p> <p>These concepts form the core steps and techniques in solving a process mining problem, guiding the analysis from raw data to actionable improvements.</p>"},{"location":"concepts/logging-best-practices/","title":"Event Logging Best Practices","text":"<p>To achieve high-quality process mining, effective event logging is crucial. Here are some best practices for event logging, along with recommendations for logging levels and necessary IDs to tie events back to a workflow process:</p>"},{"location":"concepts/logging-best-practices/#best-practices-for-event-logging","title":"Best Practices for Event Logging","text":"<ol> <li> <p>Granular Logging:</p> <ul> <li>Log events at a granular level to capture detailed information about each step in a workflow. This includes start and end times, user interactions, system decisions, and any transitions between states.</li> <li> <p>Consistent and Structured Log Format:</p> </li> <li> <p>Ensure that logs are structured consistently across all systems involved. Use a common format (e.g., JSON, XML) that is easily parsable and allows for the inclusion of detailed metadata.</p> </li> <li> <p>Timestamp Precision:</p> </li> <li> <p>Use high-precision timestamps (e.g., microseconds) to accurately capture the sequence of events, especially in high-frequency processes.</p> </li> <li> <p>Unique Identifiers:</p> </li> <li> <p>Assign unique identifiers to various components of the process:</p> <ul> <li>Process Instance ID: Ties all events related to a specific workflow instance together.</li> <li>Activity ID: Identifies specific activities or steps within the process.</li> <li>Case ID: A broader identifier that might encompass multiple process instances if they belong to the same case or customer.</li> <li>User ID: Identifies the user responsible for triggering or interacting with the event.</li> <li>Correlation ID: Used to track events across distributed systems, ensuring end-to-end visibility of a workflow that spans multiple services.</li> </ul> </li> <li> <p>Contextual Information:</p> </li> <li> <p>Include contextual information in the logs, such as system state, user inputs, decision outcomes, and environmental variables, to provide insights into why certain events occurred.</p> </li> <li> <p>Error and Exception Logging:</p> </li> <li> <p>Capture detailed information about errors and exceptions, including stack traces, error codes, and contextual data to facilitate root cause analysis.</p> </li> <li> <p>Event Sequencing and Correlation:</p> </li> <li> <p>Ensure that logs can be easily sequenced and correlated to reconstruct the exact flow of the process. This is crucial for identifying bottlenecks, loops, or deviations from the expected process flow.</p> </li> <li> <p>Data Privacy and Security:</p> </li> <li> <p>Be mindful of logging sensitive information. Anonymize or obfuscate personal data where necessary, and ensure logs are stored securely.</p> </li> <li> <p>Scalable Storage and Retrieval:</p> </li> <li> <p>Implement a logging system that can scale with your process volume and provides efficient retrieval mechanisms for large-scale analysis. Consider using distributed logging systems or cloud-based solutions.</p> </li> <li> <p>Real-Time Monitoring and Alerts:</p> </li> <li> <p>Set up real-time monitoring and alerting on specific log events to detect and respond to anomalies or issues as they occur.</p> </li> </ul> </li> </ol>"},{"location":"concepts/logging-best-practices/#logging-levels-for-process-mining","title":"Logging Levels for Process Mining","text":"<ul> <li>DEBUG:<ul> <li>Capture detailed information useful for diagnosing problems. This should include low-level event details that help in reconstructing the exact sequence of actions.</li> </ul> </li> <li>INFO:<ul> <li>Log high-level events that describe the normal operation of the process, such as process start and end, key decision points, and successful completions of significant tasks.</li> </ul> </li> <li>WARN:<ul> <li>Record potentially harmful situations that do not stop the process but might lead to issues, such as retries, minor errors, or unexpected delays.</li> </ul> </li> <li>ERROR:<ul> <li>Capture events where the process encounters problems that require intervention but can continue running, such as failed tasks that are retried successfully.</li> </ul> </li> <li>FATAL:<ul> <li>Log events that cause the process to terminate or fail entirely. This includes critical errors that stop the process flow.</li> </ul> </li> </ul>"},{"location":"concepts/logging-best-practices/#key-identifiers-in-event-logs","title":"Key Identifiers in Event Logs","text":"<ul> <li> <p>Process Instance ID: Links all events associated with a specific instance of the process, ensuring the ability to trace the entire workflow.</p> </li> <li> <p>Activity ID: Identifies the specific task or step within the process, helping to isolate where in the process an event occurred.</p> </li> <li> <p>Case ID: Allows for grouping of related process instances, useful in scenarios like customer support where multiple processes may be related to the same case.</p> </li> <li> <p>User ID: Ties events to the individual user or system that triggered or interacted with the process, aiding in user behavior analysis and auditing.</p> </li> <li> <p>Correlation ID: Essential in distributed systems to track related events across different services and systems, ensuring you can trace a process end-to-end.</p> </li> </ul> <p>By following these best practices and using appropriate logging levels and identifiers, you can ensure that your event logs provide the necessary detail and structure for effective process mining, leading to more accurate and actionable insights.</p>"},{"location":"concepts/logging-best-practices/#standards-organizations","title":"Standards Organizations","text":"<p>Promoting and adhering to best practices in event logging, particularly for process mining, is often guided by industry standards, frameworks, and guidelines. Several organizations and websites play a key role in defining and promoting these best practices:</p>"},{"location":"concepts/logging-best-practices/#ieee-institute-of-electrical-and-electronics-engineers","title":"IEEE (Institute of Electrical and Electronics Engineers)","text":"<ul> <li> <p>IEEE Standards: IEEE offers various standards related to software engineering, including logging practices. Standards like IEEE 1012 (Standard for System and Software Verification and Validation) provide guidelines on what and how to log to ensure traceability and auditability.</p> </li> <li> <p>Website: IEEE Standards</p> </li> </ul>"},{"location":"concepts/logging-best-practices/#iso-international-organization-for-standardization","title":"ISO (International Organization for Standardization)","text":"<ul> <li> <p>ISO/IEC 27001: This standard specifies the requirements for establishing, implementing, maintaining, and continually improving an information security management system. It includes guidelines on logging and monitoring for security purposes.</p> </li> <li> <p>ISO/IEC 19510: Known as BPMN (Business Process Model and Notation), this standard provides a graphical notation for specifying business processes in a Business Process Diagram (BPD). While not specific to logging, it influences how processes are documented and monitored, which impacts logging strategies.</p> </li> <li> <p>Website: ISO Standards</p> </li> </ul>"},{"location":"concepts/logging-best-practices/#3-nist-national-institute-of-standards-and-technology","title":"3 NIST (National Institute of Standards and Technology)","text":"<ul> <li> <p>NIST SP 800-92: This is a guide to computer security log management. It provides comprehensive guidelines on developing, implementing, and maintaining effective log management programs, which are critical for process mining and security.</p> </li> <li> <p>Website: NIST SP 800-92</p> </li> </ul>"},{"location":"concepts/logging-best-practices/#4-wasp-open-web-application-security-project","title":"4 *WASP (Open Web Application Security Project)","text":"<ul> <li> <p>OWASP Logging Cheat Sheet: OWASP provides best practices for secure logging, especially in web applications. It covers the essential aspects of what to log, how to structure logs, and how to ensure logs are useful for security and process analysis.</p> </li> <li> <p>Website: OWASP Logging Cheat Sheet</p> </li> </ul>"},{"location":"concepts/logging-best-practices/#w3c-world-wide-web-consortium","title":"W3C (World Wide Web Consortium)","text":"<ul> <li> <p>W3C Web Performance Working Group: While primarily focused on web performance, the guidelines and recommendations from W3C on monitoring and performance logging can be extended to cover process mining scenarios, especially in web-based applications.</p> </li> <li> <p>Website: W3C Web Performance</p> </li> <li> <p>W3C Data Provinance</p> </li> </ul>"},{"location":"concepts/logging-best-practices/#itil-information-technology-infrastructure-library","title":"ITIL (Information Technology Infrastructure Library)","text":"<ul> <li> <p>ITIL Framework: ITIL is a widely recognized framework for IT service management. It provides guidelines on best practices for service lifecycle management, including logging for incident management, change management, and continuous improvement.</p> </li> <li> <p>Website: ITIL</p> </li> </ul>"},{"location":"concepts/logging-best-practices/#apqc-american-productivity-quality-center","title":"APQC (American Productivity &amp; Quality Center)","text":"<ul> <li> <p>Process Classification Framework (PCF): APQC provides a taxonomy of business processes across various industries. This framework helps organizations standardize their process documentation and monitoring, indirectly influencing how event logging should be structured for process mining.</p> </li> <li> <p>Website: APQC Process Classification Framework</p> </li> </ul>"},{"location":"concepts/logging-best-practices/#elk-stack-elasticsearch-logstash-kibana-and-observability-platforms","title":"ELK Stack (Elasticsearch, Logstash, Kibana) and Observability Platforms","text":"<ul> <li> <p>Best Practices Documentation: While not a standards organization, the ELK Stack (and similar platforms like Splunk and Datadog) provides extensive documentation and community best practices on log management, structuring logs for analysis, and implementing logging strategies that facilitate effective process mining.</p> </li> <li> <p>Website: Elastic.co</p> </li> </ul>"},{"location":"concepts/logging-best-practices/#bpmiorg-business-process-management-initiative","title":"BPMI.org (Business Process Management Initiative)","text":"<ul> <li> <p>Business Process Modeling Notation (BPMN): This initiative developed BPMN, which provides a standard way to visualize business processes. The notation helps standardize the documentation of processes, which influences the structure and quality of event logs.</p> </li> <li> <p>Website: BPMI.org</p> </li> </ul>"},{"location":"concepts/logging-best-practices/#opentelemetry","title":"OpenTelemetry","text":"<ul> <li> <p>OpenTelemetry Project: This is an open-source observability framework that provides APIs and tools to instrument, generate, collect, and export telemetry data (logs, metrics, and traces) to help you analyze your software's performance and behavior. OpenTelemetry helps standardize how logging is done, which is beneficial for process mining.</p> </li> <li> <p>Website: OpenTelemetry</p> </li> </ul> <p>By following the guidelines and standards promoted by these organizations, companies can ensure that their logging practices are robust, consistent, and aligned with best practices, ultimately supporting high-quality process mining and analysis.</p> <p>4o</p> <p>ChatGPT can make mistakes. Check important info.</p>"},{"location":"concepts/sales-pipeline/","title":"Sales Pipeline Example","text":"<p>The sales pipeline is a good example of a process that can benefit from process mining.</p>"},{"location":"concepts/sales-pipeline/#stages-in-an-enterprise-business-sale","title":"Stages in an Enterprise Business Sale","text":""},{"location":"concepts/sales-pipeline/#1-lead","title":"1.  Lead","text":"<p>A potential customer who has shown some initial interest in the product or service. Leads are usually identified through marketing efforts such as website visits, email campaigns, or social media interactions.</p>"},{"location":"concepts/sales-pipeline/#2-qualified-lead","title":"2.  Qualified Lead","text":"<p>A lead that has been vetted and determined to have a genuine interest in the product, a need that the product can fulfill, and the potential to make a purchase. This stage involves assessing the lead's fit based on criteria such as budget, authority, need, and timeline (BANT).</p>"},{"location":"concepts/sales-pipeline/#3-opportunity","title":"3.  Opportunity","text":"<p>A qualified lead that has moved further down the pipeline and is actively engaged in discussions with the sales team. At this stage, the potential customer's needs and pain points are well understood, and the sales team works to align the product offering with those needs.</p>"},{"location":"concepts/sales-pipeline/#4-proposal","title":"4.  Proposal","text":"<p>The stage where the sales team presents a formal proposal to the potential customer, outlining the product or service offering, pricing, terms, and other relevant details. This is often accompanied by a detailed sales presentation or demo.</p>"},{"location":"concepts/sales-pipeline/#5-negotiation","title":"5.  Negotiation","text":"<p>In this stage, both the sales team and the potential customer negotiate the terms of the deal. This may involve discussions on pricing, contract terms, customization options, and other specific requirements. The goal is to reach a mutually beneficial agreement.</p>"},{"location":"concepts/sales-pipeline/#6-closed-won","title":"6.  Closed-Won","text":"<p>This stage is reached when the customer agrees to the terms and signs a contract or makes a purchase. The deal is considered \"won,\" and the lead officially becomes a customer. At this point, the focus shifts to delivering the promised product or service.</p>"},{"location":"concepts/sales-pipeline/#7-implementation","title":"7.  Implementation","text":"<p>The process of delivering the product or service to the customer begins. This might involve setting up the product, providing training, or integrating the solution with the customer's existing systems.</p>"},{"location":"concepts/sales-pipeline/#8-production","title":"8.  Production","text":"<p>The final stage, where the product or service is fully operational and in use by the customer. Ongoing support, maintenance, and relationship management are essential to ensure customer satisfaction and foster long-term loyalty.</p> <p>These stages represent the typical flow of a sales pipeline, guiding the sales team through the process of converting leads into customers and ensuring a smooth transition from sales to production.</p>"},{"location":"concepts/sales-pipeline/#key-performance-metrics-for-a-sales-pipeline","title":"Key Performance Metrics for a Sales Pipeline","text":"<p>Key Performance Indicators (KPIs) for a sales pipeline are metrics that help measure the effectiveness and efficiency of the sales process. These KPIs provide insights into how well leads are being managed, the performance of the sales team, and the overall health of the sales pipeline. Here are some common KPIs for a sales pipeline:</p>"},{"location":"concepts/sales-pipeline/#1-number-of-leads","title":"1: Number of Leads","text":"<p>The total number of new leads entering the sales pipeline over a specific period. This KPI indicates the effectiveness of marketing efforts and the potential for future sales. A higher number of leads suggests a healthy influx of potential customers.</p>"},{"location":"concepts/sales-pipeline/#2-lead-conversion-rate","title":"2: Lead Conversion Rate","text":"<p>The percentage of leads that are successfully converted into qualified leads. This KPI measures the effectiveness of the lead qualification process and helps identify how well the sales team is able to turn initial interest into genuine sales opportunities.</p>"},{"location":"concepts/sales-pipeline/#3-opportunity-conversion-rate","title":"3: Opportunity Conversion Rate","text":"<p>The percentage of qualified leads that are converted into sales opportunities. This KPI reflects the sales team's ability to move prospects further down the pipeline, turning them into active opportunities for a sale.</p>"},{"location":"concepts/sales-pipeline/#4-win-rate-close-rate","title":"4: Win Rate (Close Rate)","text":"<p>The percentage of opportunities that result in a closed-won deal. This KPI measures the effectiveness of the sales process in converting opportunities into actual sales. A higher win rate indicates a more efficient and successful sales process.</p>"},{"location":"concepts/sales-pipeline/#5-average-deal-size","title":"5: Average Deal Size","text":"<p>The average value of deals closed over a specific period. This KPI helps assess the revenue potential of the sales pipeline and can indicate the quality of the leads being pursued.</p>"},{"location":"concepts/sales-pipeline/#6-sales-cycle-length","title":"6: Sales Cycle Length","text":"<p>The average time it takes to move a lead from the initial stage to a closed-won deal. This KPI helps measure the efficiency of the sales process. A shorter sales cycle length typically indicates a more efficient pipeline, while a longer cycle may suggest delays or inefficiencies.</p>"},{"location":"concepts/sales-pipeline/#7-pipeline-velocity","title":"7: Pipeline Velocity","text":"<p>A measure of how quickly deals move through the sales pipeline, calculated as the number of opportunities multiplied by the average deal size, win rate, and divided by the length of the sales cycle. This KPI combines several key metrics to provide a comprehensive view of the pipeline's momentum, indicating how quickly revenue is likely to be generated.</p>"},{"location":"concepts/sales-pipeline/#8-sales-pipeline-coverage","title":"8: Sales Pipeline Coverage","text":"<p>The ratio of the value of opportunities in the pipeline to the revenue target for a given period. This KPI helps assess whether the pipeline has enough potential value to meet future sales targets. A pipeline coverage ratio of 3:1 is often considered healthy.</p>"},{"location":"concepts/sales-pipeline/#9-lead-response-time","title":"9: Lead Response Time","text":"<p>The average time it takes for the sales team to follow up with a new lead. This KPI is critical for ensuring that leads are engaged promptly, which can significantly impact conversion rates and customer satisfaction.</p>"},{"location":"concepts/sales-pipeline/#10-lost-opportunity-analysis","title":"10: Lost Opportunity Analysis","text":"<p>The percentage of opportunities that were lost and the reasons behind those losses. This KPI helps identify common obstacles or challenges that prevent deals from closing, providing insights for process improvements and training needs.</p>"},{"location":"concepts/sales-pipeline/#11-customer-acquisition-cost-cac","title":"11: Customer Acquisition Cost (CAC)","text":"<p>The total cost of acquiring a new customer, including marketing and sales expenses, divided by the number of new customers acquired. This KPI is crucial for understanding the efficiency of the sales process and ensuring that the cost to acquire customers is aligned with the expected revenue and profitability.</p>"},{"location":"concepts/sales-pipeline/#12-customer-lifetime-value-cltv","title":"12: Customer Lifetime Value (CLTV)","text":"<p>The total revenue expected from a customer over the duration of their relationship with the company. This KPI helps measure the long-term value of customers acquired through the sales pipeline and can inform decisions about resource allocation and customer retention strategies.</p>"}]}